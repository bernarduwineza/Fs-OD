%==============================================================================
% This is a document for writing the final report for EE260 project. 
%
% It uses the NeurIPS (formerly NIPS) style. When making changes, please refer 
% to the style file and example given (../nips_style/neurips_2020.tex)

% Please leave comments as you see it necessary; it will help others 
% who also need to modify the document. 
% -JBU
%==============================================================================

\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[preprint, nonatbib]{./../nips_style/neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\usepackage{url}
\urlstyle{same}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\input{./sym_def.tex}
\title{Few Shot Object Detection Using Fine-tuning}

\author{% Please fill in your info
  Jean-Bernard Uwineza 
  %\thanks{Use footnote for providing further information
  %  about author (webpage, alternative address)---\emph{not} for acknowledging
  %  funding agencies.} 
  \\
  %Department of Electrical Engineering \\
  University of California, Riverside\\
  Riverside, CA 92501 \\
  \texttt{buwineza@ee.ucr.edu} \\
  \And
  Chetan Reddy Mudireddy \\
  University of California, Riverside \\
  Riverside, CA 92501 \\
  \texttt{cmudi001@ucr.edu} \\
  \AND
  %
  Om Shankar Ohdar \\
  University of California, Riverside \\
  Riverside, CA 92501 \\
  \texttt{oohda001@ucr.edu} \\
  \And
  %
  Sayak Nag \\
  University of California,Riverside \\
  Riverside, CA 92501 \\
  \texttt{snag005@ucr.edu} \\
  %
  \And
  Vikarn Bhakri \\
  University of California, Riverside \\
  Riverside, CA 92501 \\
  \texttt{vbhak001@ucr.edu} \\
}

\begin{document}

\maketitle

\begin{abstract} 

Abstract goes here

\end{abstract}

\subsection{Some rules for collaboration}

\begin{itemize}
  \item DO NOT delete text you did not write, unless the person who wrote it asks you. 
  If you have issues with some of the text, discuss with the author first. 
 
  \item Commit and push to GitHub often. This helps others staying updated. 

  \item Apply other common sense rules of collaboration.

\end{itemize}

\section{Introduction}
{\color{magenta}
\textbf{This text is from the proposal. It will be changed.} \\
Few-shot learning has received significant interest in the past few years, 
but mainly for the tasks of classification and rarely for object detection. 
In computer vision, the task of object detection is more challenging since the detector 
not only has to perform recognition of the different kinds of objects present,
it also has to localize them. This is already a challenging task that relies heavily 
on the availability of massive amounts of labeled training data. Now when a new data-point 
is obtained belonging to a novel category, adapting the model becomes a very difficult 
task especially when the new category contains a few samples. Recently, meta learning techniques 
have been proposed for adapting deep models to novel categories. However, they are not easily 
extendable to the task of object detection. Take for example the Matching \cite{VinyalsBLKW16} 
and Prototypical Networks \cite{snell2017prototypical}, building prototypes of objects is 
much more difficult than building prototype of the categories. Another approach that is being 
explored by researchers is to provide ways to fine-tune the detection layers of 
deep models to adapt to the new categories \cite{wang2020frustratingly}.


In this project we aim to do a comparative study of meta-learning and fine-tuning approaches 
towards object detection. We aim to experiment on benchmark datasets such as 
COCO \cite{LinMBHPRDZ14} and PASCAL \cite{Everingham10} and also extend these approaches 
towards 3D object detection with the KITTI dataset \cite{Geiger2013IJRR}.

In addition, we plan on examining how many shots are necessary to reach comparable accuracy relative to 
conventional detection approaches. To this end, we will attempt to develop a metric that assesses
the model's knowledge, and requests additional labeled examples if it has not reached a certain accuracy. 
This will allow us to further compare the performance of the two approaches. 
} 

\section{Few-shot Finetuning}

\subsection{Problem Definition}
Suppose there is  a set of base classes $C_b$ with \textit{sufficiently many} training examples and a set $C_n$ 
of novel classes with only $K$ examples per class (in this paper $K \leq 10$ always). 
The few-shot datasets used herein are balanced, meaning that for each novel class there are $K$ annotated examples.
When $K$ examples are used in finetuning, we call it a $k$-shot detection. 
Denote the set of training images as $\mathcal{X}$ and their corresponding labels as $\mathcal{Y}$. 
A detection dataset $\mathcal{D} = \{ (x,y), ~ x \in \mathcal{X}, ~ y \in \mathcal{Y} \}$, where $x$ 
is the input image and $y= \{(c_i, \bm 1_i) \}_{i=1}^N$ denotes the categories $c \in  C_b \cup C_n$ and 
bounding box coordinates $\1 $ of the $N$ instances of the object in the image $x$. 

In \cite{VinyalsBLKW16} and \cite{snell2017prototypical}, as is usually used in few-shot learning, 
the authors use $N$-way $K$-shot regime for evaluation, where $N$ is the number of training examples, 
and $K$ is the number of classes in the training set. 
In this paper, the detector model is evaluated on a test set that includes base and novel classes
to optimize the detection accuracy as measured by mean average precision (mAP) of both novel and base classes.

\begin{figure}[h]
  % \centering
  \begin{minipage}{0.47\textwidth}
  % \fbox{
  \includegraphics[width=\textwidth, height=0.17\textheight]{./../../figures/fs_det/s1_finetuning.png}
  % }
  \subcaption{Stage I}
  \label{stage-1-finetuning}
  \end{minipage}
  %
  \begin{minipage}{0.47\textwidth}
  % \fbox{
  \includegraphics[width=\textwidth, height=0.17\textheight]{./../../figures/fs_det/s2_finetuning.png}
  % }
  \subcaption{Stage II}
  \label{stage-2-finetuning}
  \end{minipage}
  \caption{Stage I \& II of  the finetuning process. Figure courtesy of Wang et al. }
  \label{finetuning}
\end{figure}

\subsection{Two-stage Finetuning}
In a two-stage process, we adopt the simple finetuning approach of \cite{wang2020frustratingly}. 
This approach builds on Faster R-CNN \cite{ren2015faster}, a popular two-stage object detector model. 
The approach is pictures in Figure \ref{finetuning}.  
Figure \ref{stage-1-finetuning} shows that the Faster R-CNN model has three main components, namely, 
the backbone, the Region Proposal Network (RPN), and the two-layer fully-connected network as a 
Region of Interest (RoI) feature extractor.  
The backbone used in this paper is the ResNet architecture \cite{he2016deep}, but it could be any other 
architecture such as the VGG16. The ResNet was chosen because it is relatively easier to optimize,
hence potentially easier to adapt to novel classes. 
In addition to Faster R-CNN model, henceforth denoted $\mathcal{F}$, the approach also uses a box 
classifier $\mathcal{C}$ for classifying objects into different classes and a box regressor $\mathcal{R}$
to predict the bounding boxes. 

The main intuitive assumption is that $\mathcal{F}$ is able to transfer the feature function $\mathcal{F}(x)$ learned from base classes to 
novel classes. In other words, $\mathcal{F}$ is class-agnostic. This leads to the main contribution of 
\cite{wang2020frustratingly}: a suggestion that when introducing novel classes, it matters 
little to retrain $\mathcal{F}$. Instead, only the the box classifier $\mathcal{C}$ and regressor $\mathcal{R}$
need to be finetuned on the examples of novel classes since the features of base classes learned with $\mathcal{F}$ 
will easily transfer to novel classes. In this way, feature representation learning and box 
prediction learning are separated into two stages, as shown in Figure \ref{finetuning}.

\textbf{Stage I: Base Training. } 
The feature  extractor and the box predictor are trained on a dataset containing 
base classes, $C_b$. The training loss function \cite{ren2015faster} is 
\begin{equation}
  L = L_{\mathcal{F}} + L_{\mathcal{C}} + L_{\mathcal{R}}. 
  \label{loss_func}
\end{equation}
The $L_{\mathcal{F}}$ loss is obtained from the output of the RPN network, hence, it helps distinguishing 
background from foreground. The $L_{\mathcal{C}}$ loss is a cross-entropy loss of the box classifier, and 
$L_{\mathcal{R}}$ is the robust loss function (smoothed $L_1$) of the box regressor. More details on $L$ can be
found in \cite{ren2015faster} and \cite{girshick2015fast}. 

\textbf{Stage II: Finetuning. }
In this stage, a small training set containing $K$ examples per class, with both base and novel classes. 
Using a smaller learning rate\footnote{The finetuning learning rate is reduced by at least 20 times compared 
to base training learning rate. In our experiments, it was reduced from $2\cdot10^{-2}$ to $1\cdot10^{-3}$.} 
and the same loss function as in \eqref{loss_func}, the box classifier 
and regressor are initialized with random weights for the novel classes and are finetuned 
while the feature extractor $\mathcal{F}$ is fixed and unmodified. 


\subsection{Euclidean Distance-based Similarity}
The weight matrix $W \in \RR^{d \times c}$ of the box classifier $\mathcal{C}$ can be expressed as 
  $W = [w_1, w_2, \dots, w_c]$ 
where the row-vector  $w_c \in \RR^d$ is the weight vector of each class.  
Recent works (such as \cite{VinyalsBLKW16,chen2019closer,qi2018low,gidaris2018dynamic}) have proposed 
box classifiers based on cosine similarity. The output of the classifier is a scaled similarity score matrix 
of the input features and the weight vectors of the classes. Specifically
\begin{equation}
  s_{ij} = \frac{\alpha \mathcal{F}(x)_i^\top w_j}{\| \mathcal{F}(x)_i \| \|w_j\|},
\end{equation}
where $\alpha$ is the scaling factor\footnote{In \cite{wang2020frustratingly}, a fixed value 
of $\alpha = 20$ was used.}. 
Cosine similarity is attractive because it offers feature normalization which reduces the 
variance between classes and improves detection accuracy for novel classes \cite{wang2020frustratingly}. 

We now propose a different similarity score, in which the Euclidean distance between the 
input features and class weight vectors are passed through the Softmax function. Specifically
\begin{equation}
  s_{ij} = \frac{e^{-\|\mathcal{F}(x)_i - w_j\|_2^2}}{\sum_{j} e^{-\|\mathcal{F}(x)_i - w_j\|_2^2}}.
\end{equation}
This similarity score provides a stronger kind of feature normalization which should help the model 
adapt to novel classes when introducing novel classes. 


\section{Other Sections}

\section{Experimental Results}

\section{Conclusion}



\if{ %This is to remove the proposal from output
\section{Problem Statement}
A small child visiting a zoo with parents for the first time recognizes a strange animal---a zebra, 
she is told. She keeps walking and a few minutes later, by the infirmary, she recognizes another animal,
only smaller and wobbly this time. ``A baby zebra'', she exclaims. From just this one example, 
she will be able to recognize just about every zebra she will ever see. Not only this, 
she will also be able to make remarkable connections to other animal that are are similar 
to zebras \cite{samuelson2005they}. 

This is an ability machines have yet to acquire. Although machines have surpassed humans in 
visually recognizing objects, they still lack the ability to do so from a few examples. 
Recently,there have been promising advances towards the goal of making machines generalize 
from a few examples via a deep learning method called \textit{few-shot learning}. 
There are many important applications that could benefit from the ability to learn from a few 
samples. Like any other problem, there are various approaches to this this task.
In this project, we propose to evaluate and analyze two of the most promising approaches. 

Few-shot learning has received significant interest in the past few years, 
but mainly for the tasks of classification and rarely for object detection. 
In computer vision, the task of object detection is more challenging since the detector 
not only has to perform recognition of the different kinds of objects present,
it also has to localize them. This is already a challenging task that relies heavily 
on the availability of massive amounts of labeled training data. Now when a new data-point 
is obtained belonging to a novel category, adapting the model becomes a very difficult 
task especially when the new category contains a few samples. Recently, meta learning techniques 
have been proposed for adapting deep models to novel categories. However, they are not easily 
extendable to the task of object detection. Take for example the Matching \cite{VinyalsBLKW16} 
and Prototypical Networks \cite{snell2017prototypical}, building prototypes of objects is 
much more difficult than building prototype of the categories. Another approach that is being 
explored by researchers is to provide ways to fine-tune the detection layers of 
deep models to adapt to the new categories \cite{wang2020frustratingly}.

In this project we aim to do a comparative study of meta-learning and fine-tuning approaches 
towards object detection. We aim to experiment on benchmark datasets such as 
COCO \cite{LinMBHPRDZ14} and PASCAL \cite{Everingham10} and also extend these approaches 
towards 3D object detection with the KITTI dataset \cite{Geiger2013IJRR}.

In addition, we plan on examining how many shots are necessary to reach comparable accuracy relative to 
conventional detection approaches. To this end, we will attempt to develop a metric that assesses
the model's knowledge, and requests additional labeled examples if it has not reached a certain accuracy. 
This will allow us to further compare the performance of the two approaches. 

\section{Work Plan}

We plan to work on the meta-learning and fine-tuning based approaches simultaneously. 
Sayak and Bernard will focus on meta-learning-based approaches, while Om, Chetan and Vikarn will work 
on fine-tuning approaches. The experimental results will be compiled by Om and Vikarn while Sayak, 
Bernard and Chetan will work on final report compilation. 
}
\fi 























\bibliographystyle{IEEEtran}
\bibliography{References}


\end{document}

